import os
import json
import requests
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

# åˆå§‹åŒ–å®¢æˆ·ç«¯
llm_client = OpenAI(
    api_key=os.getenv("LLM_API_KEY"),
    base_url=os.getenv("LLM_BASE_URL")
)

SERPER_API_KEY = os.getenv("SERPER_API_KEY")
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")

def search_with_serper(query, num_results=10):
    """
    ä½¿ç”¨ Serper.dev API è¿›è¡Œæœç´¢
    """
    if not SERPER_API_KEY:
        return None

    url = "https://google.serper.dev/search"
    headers = {
        "X-API-KEY": SERPER_API_KEY,
        "Content-Type": "application/json"
    }
    payload = {
        "q": query,
        "num": num_results,
        "gl": "cn",  # åœ°ç†ä½ç½®: ä¸­å›½
        "hl": "zh-cn"  # è¯­è¨€: ä¸­æ–‡
    }

    try:
        response = requests.post(url, json=payload, headers=headers, timeout=10)
        if response.status_code == 200:
            return response.json()
        else:
            print(f"âš ï¸ Serper API è¿”å›é”™è¯¯: {response.status_code}")
            return None
    except Exception as e:
        print(f"âš ï¸ Serper API è¯·æ±‚å¤±è´¥: {e}")
        return None

def search_with_tavily(query, max_results=10):
    """
    ä½¿ç”¨ Tavily AI API è¿›è¡Œæœç´¢
    """
    if not TAVILY_API_KEY:
        return None

    url = "https://api.tavily.com/search"
    headers = {
        "Content-Type": "application/json"
    }
    payload = {
        "api_key": TAVILY_API_KEY,
        "query": query,
        "max_results": max_results,
        "search_depth": "advanced",
        "include_answer": True,
        "include_raw_content": False
    }

    try:
        response = requests.post(url, json=payload, headers=headers, timeout=15)
        if response.status_code == 200:
            return response.json()
        else:
            print(f"âš ï¸ Tavily API è¿”å›é”™è¯¯: {response.status_code}")
            return None
    except Exception as e:
        print(f"âš ï¸ Tavily API è¯·æ±‚å¤±è´¥: {e}")
        return None

def format_search_results(serper_data=None, tavily_data=None):
    """
    ç»Ÿä¸€æ ¼å¼åŒ–æœç´¢ç»“æœ
    """
    formatted_results = []

    # å¤„ç† Serper ç»“æœ
    if serper_data and "organic" in serper_data:
        for item in serper_data["organic"][:10]:
            formatted_results.append({
                "title": item.get("title", ""),
                "snippet": item.get("snippet", ""),
                "url": item.get("link", ""),
                "source": "serper"
            })

    # å¤„ç† Tavily ç»“æœ
    if tavily_data and "results" in tavily_data:
        for item in tavily_data["results"][:10]:
            formatted_results.append({
                "title": item.get("title", ""),
                "snippet": item.get("content", ""),
                "url": item.get("url", ""),
                "source": "tavily"
            })

    return formatted_results

def summarize_with_llm(query, search_results):
    """
    ä½¿ç”¨ LLM æ€»ç»“æœç´¢ç»“æœ
    """
    # æ„å»ºæœç´¢ç»“æœæ–‡æœ¬
    results_text = "\n\n".join([
        f"ã€{i+1}ã€‘{r['title']}\n{r['snippet']}\næ¥æº: {r['url']}"
        for i, r in enumerate(search_results[:10])
    ])

    system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–°é—»åˆ†æå¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯ä»æœç´¢ç»“æœä¸­æå–å…³é”®ä¿¡æ¯ï¼Œå¹¶ä»¥ç»“æ„åŒ–çš„JSONæ ¼å¼è¿”å›ã€‚

è¦æ±‚ï¼š
1. æå–äº‹ä»¶çš„å…³é”®äº‹å®
2. æ¢³ç†æ—¶é—´çº¿ï¼ˆèµ·å› ã€å‘å±•ã€å½±å“ï¼‰
3. è¯†åˆ«å…³é”®äººç‰©/æœºæ„
4. åˆ¤æ–­èˆ†æƒ…å€¾å‘ï¼ˆpositive/negative/neutralï¼‰
5. æ’°å†™200å­—ç»¼è¿°

è¿”å›æ ¼å¼å¿…é¡»æ˜¯æœ‰æ•ˆçš„JSONï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—ã€‚"""

    user_prompt = f"""è¯·åˆ†æä»¥ä¸‹å…³äº"{query}"çš„æœç´¢ç»“æœï¼Œå¹¶è¿”å›JSONæ ¼å¼çš„åˆ†æï¼š

{results_text}

è¯·è¿”å›ä»¥ä¸‹JSONç»“æ„ï¼š
{{
  "key_facts": ["äº‹å®1", "äº‹å®2", "äº‹å®3"],
  "timeline": {{
    "cause": "äº‹ä»¶èµ·å› ï¼ˆ60-80å­—ï¼‰",
    "development": "å‘å±•è¿‡ç¨‹ï¼ˆ60-80å­—ï¼‰",
    "impact": "å½±å“/ç»“æœï¼ˆ60-80å­—ï¼‰"
  }},
  "key_actors": ["ä¸»ä½“1", "ä¸»ä½“2"],
  "sentiment": "positive/negative/neutral",
  "summary": "200å­—ç»¼è¿°",
  "sources": ["{search_results[0]['url'] if search_results else ''}", "{search_results[1]['url'] if len(search_results) > 1 else ''}"]
}}"""

    try:
        response = llm_client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.3,
            response_format={"type": "json_object"}
        )

        result_text = response.choices[0].message.content
        return json.loads(result_text)

    except Exception as e:
        print(f"âŒ LLM åˆ†æå¤±è´¥: {e}")
        return None

def research_topic(topic, date=None):
    """
    ä¸»å…¥å£ï¼šæœç´¢ + æ€»ç»“
    Returns: {
        "key_facts": [...],
        "timeline": {...},
        "key_actors": [...],
        "sentiment": "positive/negative/neutral",
        "summary": "200å­—ç»¼è¿°",
        "sources": ["url1", "url2", ...]
    }
    """
    print(f"ğŸ” å¼€å§‹ç ”ç©¶ä¸»é¢˜: {topic}")

    # æ„å»ºæœç´¢æŸ¥è¯¢
    search_query = f"{topic} æ–°é—»" if date is None else f"{topic} {date} æ–°é—»"

    # å°è¯•å¤šä¸ªæœç´¢æº
    serper_result = None
    tavily_result = None

    if SERPER_API_KEY:
        print("  - ä½¿ç”¨ Serper.dev æœç´¢...")
        serper_result = search_with_serper(search_query)

    if TAVILY_API_KEY and not serper_result:
        print("  - ä½¿ç”¨ Tavily AI æœç´¢...")
        tavily_result = search_with_tavily(search_query)

    # æ ¼å¼åŒ–æœç´¢ç»“æœ
    search_results = format_search_results(serper_result, tavily_result)

    if not search_results:
        print("âš ï¸ æœªè·å–åˆ°æœç´¢ç»“æœï¼Œå°†ä½¿ç”¨ LLM ç”Ÿæˆå†…å®¹")
        # è¿”å›ç©ºç»“æ„ï¼Œåç»­ç”± LLM ç›´æ¥ç”Ÿæˆ
        return {
            "key_facts": [],
            "timeline": {
                "cause": "",
                "development": "",
                "impact": ""
            },
            "key_actors": [],
            "sentiment": "neutral",
            "summary": "",
            "sources": [],
            "raw_results": []
        }

    print(f"  âœ… è·å–åˆ° {len(search_results)} æ¡æœç´¢ç»“æœ")

    # ä½¿ç”¨ LLM åˆ†æ
    print("  - ä½¿ç”¨ LLM åˆ†ææœç´¢ç»“æœ...")
    analysis = summarize_with_llm(topic, search_results)

    if analysis:
        analysis["raw_results"] = search_results
        print("  âœ… ç ”ç©¶å®Œæˆ")
        return analysis
    else:
        # LLM å¤±è´¥æ—¶è¿”å›åŸå§‹ç»“æœ
        return {
            "key_facts": [r["snippet"] for r in search_results[:5]],
            "timeline": {
                "cause": "",
                "development": "",
                "impact": ""
            },
            "key_actors": [],
            "sentiment": "neutral",
            "summary": search_results[0]["snippet"] if search_results else "",
            "sources": [r["url"] for r in search_results[:5]],
            "raw_results": search_results
        }
