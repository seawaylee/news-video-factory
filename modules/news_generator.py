import os
import json
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

# é…ç½® OpenAI Client
client = OpenAI(
    base_url=os.getenv("LLM_BASE_URL", "http://127.0.0.1:8045/v1"),
    api_key=os.getenv("LLM_API_KEY")
)

def generate_news_analysis(topic, date, research_data=None):
    """
    ä½¿ç”¨ LLM ç”Ÿæˆæ–°é—»åˆ†æå†…å®¹

    :param topic: æ–°é—»ä¸»é¢˜
    :param date: æ—¥æœŸ (YYYYMMDD)
    :param research_data: ç½‘ç»œç ”ç©¶æ•°æ® (æ¥è‡ª web_researcher)
    :return: æ–°é—»åˆ†ææ•°æ®å­—å…¸
    """
    print(f"ğŸ¤– AI æ­£åœ¨åˆ†ææ–°é—»: {topic}...")

    # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
    context = ""
    if research_data and research_data.get("summary"):
        context = f"\n\nã€æœç´¢ç»“æœæ¦‚è¦ã€‘\n{research_data['summary']}\n"
        if research_data.get("key_facts"):
            context += f"\nã€å…³é”®äº‹å®ã€‘\n" + "\n".join(f"- {fact}" for fact in research_data['key_facts'][:5])

    system_prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ–°é—»åˆ†æå¸ˆ,æ“…é•¿ç”¨é€šä¿—æ˜“æ‡‚ã€ç°ä»£æ„Ÿå¼ºçš„æ–¹å¼è§£è¯»çƒ­ç‚¹äº‹ä»¶ã€‚

ã€æ ¸å¿ƒè¦æ±‚ã€‘
1. **é£æ ¼**: ç°ä»£ã€å¹½é»˜ã€æœ‰è§åœ°ã€‚**ç»å¯¹ç¦æ­¢**ä½¿ç”¨"å“¥ä»¬å„¿å§ä»¬å„¿"ã€"äº²çˆ±çš„æœ‹å‹ä»¬"ã€"å®¶äººä»¬"ç­‰è¿‡æ—¶æˆ–æ²¹è…»çš„å¼€åœºç™½ã€‚ç›´å…¥ä¸»é¢˜ï¼Œä¸è¦åºŸè¯ã€‚
2. **ç»“æ„**: ä¸‰å¹•å¼å™äº‹
   - èµ·å›  (60-80å­—): äº‹ä»¶èƒŒæ™¯å’Œè§¦å‘åŸå› 
   - å‘å±• (60-80å­—): äº‹ä»¶è¿›å±•å’Œå…³é”®è½¬æŠ˜
   - å½±å“ (60-80å­—): ç»“æœåˆ†æå’Œç¤¾ä¼šå½±å“
3. **æƒ…æ„Ÿå€¾å‘**: å‡†ç¡®åˆ¤æ–­ positive/negative/neutral
4. **è½»æ¾æ€»ç»“**: 200å­—å·¦å³çš„é€šä¿—æ˜“æ‡‚æ€»ç»“

ã€è¾“å‡ºæ ¼å¼ã€‘
ä¸¥æ ¼çš„ JSON æ ¼å¼,ä¸è¦åŒ…å« markdown ä»£ç å—æ ‡è®°:
{
  "topic": "æ–°é—»ä¸»é¢˜",
  "date": "YYYYMMDD",
  "headline": "å¸å¼•äººçš„æ ‡é¢˜(10-15å­—)",
  "timeline": {
    "cause": "èµ·å› æè¿°(60-80å­—,å£è¯­åŒ–)",
    "development": "å‘å±•æè¿°(60-80å­—,æœ‰ç”»é¢æ„Ÿ)",
    "impact": "å½±å“æè¿°(60-80å­—,è´´è¿‘ç”Ÿæ´»)"
  },
  "key_actors": ["ä¸»ä½“1", "ä¸»ä½“2"],
  "sentiment": "positive/negative/neutral",
  "sources": ["url1", "url2"],
  "casual_summary": "è½»æ¾æ€»ç»“(200å­—,è§‚ç‚¹çŠ€åˆ©,ä¸è½ä¿—å¥—,ç›´æ¥è®²äº‹)"
}
"""

    user_prompt = f"""è¯·åˆ†æä»¥ä¸‹æ–°é—»äº‹ä»¶: {topic}
æ—¥æœŸ: {date or "æœ€è¿‘"}
{context}

è¦æ±‚:
1. æ ‡é¢˜è¦ç®€æ´æœ‰åŠ›,å¸å¼•çœ¼çƒ
2. ä¸‰å¹•å¼å†…å®¹è¦åƒè®²æ•…äº‹,æœ‰ç”»é¢æ„Ÿ
3. è½»æ¾æ€»ç»“è¦é€šä¿—æ˜“æ‡‚,é¿å…å®˜è¯å¥—è¯
"""

    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",  # ä½¿ç”¨æœ¬åœ°APIæ”¯æŒçš„æ¨¡å‹å
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.7,
            response_format={"type": "json_object"}
        )

        content = response.choices[0].message.content.strip()
        # æ¸…ç†å¯èƒ½å­˜åœ¨çš„ markdown æ ‡è®°
        if content.startswith("```json"):
            content = content[7:]
        if content.endswith("```"):
            content = content[:-3]

        data = json.loads(content)

        # ç¡®ä¿å­—æ®µå­˜åœ¨
        data['topic'] = topic
        data['date'] = date or ""

        # å¦‚æœæœ‰ç ”ç©¶æ•°æ®,è¡¥å……æ¥æº
        if research_data and research_data.get("sources"):
            data['sources'] = research_data['sources'][:5]

        return data

    except Exception as e:
        print(f"âŒ æ–°é—»åˆ†æç”Ÿæˆå¤±è´¥: {e}")
        # Fallback æ•°æ®,é˜²æ­¢ç¨‹åºå´©æºƒ
        return {
            "topic": topic,
            "date": date or "",
            "headline": f"{topic}æ·±åº¦è§£è¯»",
            "timeline": {
                "cause": "AI ç”Ÿæˆå‡ºé”™,è¯·æ£€æŸ¥ API è¿æ¥ã€‚",
                "development": "AI ç”Ÿæˆå‡ºé”™,è¯·æ£€æŸ¥ API è¿æ¥ã€‚",
                "impact": "AI ç”Ÿæˆå‡ºé”™,è¯·æ£€æŸ¥ API è¿æ¥ã€‚"
            },
            "key_actors": [],
            "sentiment": "neutral",
            "sources": [],
            "casual_summary": "AI ç”Ÿæˆå‡ºé”™,è¯·æ£€æŸ¥ç½‘ç»œé…ç½®ã€‚"
        }
